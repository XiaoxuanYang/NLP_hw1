{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 0 fix AND modify a buggy Python script and scrape some job postings!\n",
    "\n",
    "Task 1: Please debug the buggy_indeed_ingest.py listed on the CourseWork website.\n",
    "\n",
    "Task 2: Modify the script such that-Your search query is NOT “data” (as shown in the script), anything else is fine but youwill use this data later in class!\n",
    "\n",
    "-Your search should not require to be fulltime\n",
    "-Please do not change the location for this assignment\n",
    "-You can add 0 to 2 more restrictions if you want (look what Indeed offers!)\n",
    "-Please have exactly 100 job postings in your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from random import uniform\n",
    "from time import sleep\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "INDEED_URL = 'https://www.indeed.com'\n",
    "NUM_PAGES = 10\n",
    "OUTPUT_FILE = './xy2346_hw1_0_data.json'\n",
    "\n",
    "request_params = {\n",
    "    'q': 'data+analyst',\n",
    "    'l': 'New York State',\n",
    "    'jt': 'fulltime',\n",
    "    'explvl': 'entry_level'}\n",
    "#add restrictions on experience level: entry level\n",
    "#change data to be \"data analyst\"\n",
    "\n",
    "job_descs = []\n",
    "for i in range(NUM_PAGES):\n",
    "    # Step 1, get the search page results\n",
    "    request_params.update({'start': i * 10})\n",
    "    indeed_response = requests.get(url=INDEED_URL + '/jobs',\n",
    "                                   params=request_params)\n",
    "\n",
    "    if indeed_response.status_code != 200:\n",
    "        print('non-200 response for search page, skipping')\n",
    "        continue\n",
    "\n",
    "    indeed_search_html = indeed_response.text\n",
    "    parsed_job_searches = BeautifulSoup(indeed_search_html, 'html.parser')\n",
    "    posting_divs = parsed_job_searches.find_all(\n",
    "        'div',\n",
    "        attrs={\"class\": [\"row\", \"result\", \"clickcard\"]})\n",
    "    job_ids = [div.attrs['data-jk'] for div in posting_divs]\n",
    "    \n",
    "    if len(job_descs) == 100:\n",
    "        break \n",
    "\n",
    "    # Get the individual job descriptions\n",
    "    for job_id in job_ids:\n",
    "        posting_response = requests.get(url=INDEED_URL + '/viewjob',\n",
    "                                            params={'jk': job_id})\n",
    "        indeed_job_html = posting_response.text\n",
    "        if posting_response.status_code != 200:\n",
    "            print('non-200 response for job description page, skipping')\n",
    "            continue\n",
    "            \n",
    "        parsed_job_post = BeautifulSoup(indeed_job_html, 'html.parser')\n",
    "        job_div = parsed_job_post.find(\n",
    "            'div',\n",
    "            attrs={'class': 'jobsearch-JobComponent-description'})\n",
    "         # Checks if there's data at all\n",
    "        if job_div:\n",
    "             job_descs.append(job_div.get_text())\n",
    "            \n",
    "        if len(job_descs) == 100:\n",
    "            break \n",
    "            \n",
    "        # DO NOT REMOVE THIS!\n",
    "        # You're scraping, this slows down your request so you\n",
    "        # do not overwhelm the site\n",
    "        sleep(uniform(1, 5))   \n",
    "            \n",
    "\n",
    "\n",
    "json.dump({'request_params': request_params,\n",
    "           'job_descriptions': job_descs},\n",
    "          open(OUTPUT_FILE, 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeed = json.load(open('./xy2346_hw1_0_data.json','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indeed['job_descriptions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1, write code to collect Tweets!\n",
    "\n",
    "2 deliverables for this assignment:\n",
    "\n",
    "-A text file named ​{YOUR_UNI}_hw1_1_conda.txt​ with the ​conda command​ you used torun the script-Data:-The data should be a JSON file named ​{YOUR_UNI}_hw1_1_data.json\n",
    "\n",
    "-The data should have exactly 100 tweets!-The data should be in the format of Twitter’s default response.-Do NOT turn in your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.read_json(elevations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
